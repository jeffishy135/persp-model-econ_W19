"""Midterm compilation of material...compiled by Zhihan Yu""""""We've covered the following topics and these will be tested:1) Differentiation2) Integration3) Dynamic Programming **There is a separate pdf to discuss theory.**4) Maximum Likelihood EstimatorI will create four classes for each topic and ideally, in the exam, all I'll need to do is create an instance of the class and it should be done almost instantly. If not, I'll justcopy paste code from here on to the jupyter notebook and make some minor modification. """from matplotlib import pyplot as plt  #import sympy as sym # Used for differentiationimport numpy as npclass Differentiation():    """    A class used to include all differentiation exercises done in problem sets.    ...    Attributes:    ------------    expr: list of str        A list of the expressions we want to conduct operations on. Each element in the list is a str.    inh_vars: str        A string of variables that are specified in the expression of interest    out_dim: int        Defines the dimensionality of the output, it's the length of the list provided in expr    in_dim: int        Defines the dimensionality of the inputs, it's the length of the variables provided in inh_vars    Methods:    ------------    _aslist(x):        converts a scalar to a vector of one-dimension.    _lambdify(lamb_expr, lamb_vars):        given the expression and the variables, lambdify the function with respect to the given variables    diff_sym(vars):        given a function (class attribute) that maps from m-dimensional input to n-dimensional output, calculate the analytical        derivatives of the function with respect to the vector of variables of interest        * supports multi-dimensional operations    diff_quo(init, step, method='forward_1'):        computes the numerical approximation of the derivative of the given function (class attribute), around the point init,        using the specified method.    """    def __init__(self, expr, inh_vars):        # This changes input string to a sympy 'function' becuase all of the following  calculations can still be        # done as though the function is a sympy object        self.expr = expr        self.inh_vars = inh_vars        self.out_dim = len(expr)  if type(expr) is list else 1        self.in_dim = 1 if ',' not in inh_vars else len(inh_vars.replace(" ", "").split(','))    def _aslist(self, x):        """        converts a scalar to a vector of one-dimension.        ...        :param x: float, int, str, or list            the value to be converted into a list        :return: list            the list of values in x        """        if x is list:            return(x)        else:            return([x])    def _evaluate(self, position):        """        Evaluate the m-dimensional funciton at a given position.        :param position: n-dimensional array            the point at which the function is calculated        :return: m-dimensional array            the output of the m-dimensional function.        """        # Make the lamb_vars into a list of strings, awaiting to be converted into sympy symbols        eval_vars = self.inh_vars.replace(" ", "")        if "," in eval_vars:            # We are dealing with m dimensional inputs            vars_slst = eval_vars.split(",")  # make each variable a str in a n-length list.        else:            # We are dealing with 1 dimensional input            vars_slst = [eval_vars]        sympysymbol_str = '['        for variable in vars_slst:            if vars_slst.index(variable) == 0:                concat_str = 'sym.symbols(\'%s\')' % (variable)            else:                concat_str = ', sym.symbols(\'%s\')' % (variable)            sympysymbol_str += concat_str        sympysymbol_str += ']'        F = sym.lambdify(eval(sympysymbol_str), np.asarray(self.expr).transpose(), 'numpy')        return(F(*position)) # if input is multidimensional    def diff_sym(self, vars):        '''This takes in the set of variables i'm differentiation with respect to        '''        # Do not allow for spaces when dealing with the variables string        vars = vars.replace(" ", "")        if self.out_dim ==1 :            expression = self.expr[0] if type(self.expr) is list else self.expr            if  "," not in vars:                # Either empty string or single variable                if len(vars)<1:                    raise ValueError("Must define variabe(s) of differentiation")                else:                    print("Single variable derivative on sympy is calculated.")                    return sym.diff(expression, sym.symbols(vars))            else:                print("Multivariate partial derivatives on sympy is calculated.")                vars_lst = vars.split(',')                defined_variables = ""                for variable in vars_lst:                    if vars_lst.index(variable)==0:                        concat = 'sym.symbols(\'' + variable + '\')'                    else:                        concat = ', sym.symbols(\'' + variable + '\')'                    defined_variables += concat                execution_cmd = 'sym.diff(sym.sympify(\'%s\'), '%(expression)  + defined_variables + ')'                return (eval(execution_cmd))        elif self.out_dim >1 :            # We have multiple outputs, so we'll have to loop through each position.            deriv_vector =[]            for f_i in self.expr:                expression = f_i                if "," not in vars:                    # Either empty string or single variable                    if len(vars) < 1:                        raise ValueError("Must define variabe(s) of differentiation")                    else:                        print("Single variable derivative on sympy is calculated.")                        deriv_vector.append( sym.diff(expression, sym.symbols(vars)))                else:                    print("Multivariate partial derivatives on sympy is calculated.")                    vars_lst = vars.split(',')                    defined_variables = ""                    for variable in vars_lst:                        if vars_lst.index(variable) == 0:                            concat = 'sym.symbols(\'' + variable + '\')'                        else:                            concat = ', sym.symbols(\'' + variable + '\')'                        defined_variables += concat                    execution_cmd = 'sym.diff(sym.sympify(\'%s\'), ' % (expression) + defined_variables + ')'                    deriv_vector.append(eval(execution_cmd))            return(deriv_vector)    def diff_quo(self, init, step, method='forward_1'):        """        Take the appropriate numerical apporximation of the derivative at a given point        :param init: array-like, list            an array defining the point on a function at which the numerical differentiation approximation is done.            dimension of array must match the input dimension m.        :param step: float            the precision of the perturbation on the variable        :param method:            the appropriate method of approximation. can be one of the following                - forward_1                - forward_2                - backward_1                - backward_2                - centered_1                - centered_2        :return: array-like, matrix            the approximated gradient of the function with respect to all its variables.        --------------        A note on the steps of what i did.        0) Initialize a matrix of zeros with dimension equal to the transpose of the jacobian            ** This is because when I change the matrix laters, it's easier just to specify the row number (which is the column after transposing)        1) Find the strides and the steps of the perturbed initial point        2) Given the perturbed initial point, evalute the function at this point        3) Given the method indicated, do the appropriate calculations        4) steps 1-3 are repeated for each column in the jacobian matrix.        5) Rows in the transposed jacobian is replaced until all have been replaced        6) Transpose and return jacobian matrix.        """        initial = np.asarray(self._aslist(init)) # is of length m (input dimensions)        initial_tup = tuple(initial[0])        if initial.shape[1] != self.in_dim:            raise ValueError("Initial point is not the correct dimension")        else:            jacobian_transposed = np.zeros([self.in_dim, self.out_dim])            for x_i in range(self.in_dim):                step_vector = step * np.eye(1, self.in_dim, x_i) # creates the standard basis vector at index (=0): (step, 0 ...) with correct dimension                stp_up = initial + step_vector                stp_down = initial - step_vector                std_up = initial + 2*step_vector                std_down = initial - 2*step_vector                stp_up_tup = tuple(stp_up[0]) # Change original ndarray to a tuple for _evaluate to work                stp_down_tup = tuple(stp_down[0])                std_up_tup = tuple(std_down[0])                std_down_tup = tuple(std_down[0])                # Given some method, the column df/ dx_i is calculated...f is a vector.                if method == 'forward_1':                    column_i = (np.asarray(self._evaluate(stp_up_tup)) - np.asarray(self._evaluate(initial_tup)))/step                if method == 'forwad_2':                    column_i = (-3*np.asarray(self._evaluate(initial_tup)) + 4*np.asarray(self._evaluate(stp_up_tup)) -                                          np.asarray(self._evaluate(std_up_tup)))/(2*step)                if method == 'backward_1':                    column_i = (np.asarray(self._evaluate(initial_tup)) - np.asarray(self._evaluate(stp_down_tup)))/step                if method == 'backward_2':                    column_i = (3*np.asarray(self._evaluate(initial_tup)) - 4*np.asarray(self._evaluate(stp_down_tup)) +                                np.asarray(self._evaluate(std_down_tup)))/(2*step)                if method == 'centered_2':                    column_i = np.asarray(self._evaluate(stp_up_tup)) - np.asarray(self._evaluate(stp_down_tup)) / (2*step)                if method == 'centered_4':                    column_i = (np.asarray(self._evaluate(std_down_tup)) - 8*np.asarray(self._evaluate(stp_down_tup)) + \                               8 * np.asarray(self._evaluate(stp_up_tup)) - np.asarray(stp_up_tup))/(12*step)                column_i = column_i.reshape(1, self.in_dim) # make it into a row vector for the jacobian matrix                # Change the ith row of the jacobian matrix into column i, replacing row is easier than replacing columns                jacobian_transposed[x_i] = column_i                # Transpose the funky jacobian matrix                jacobian = jacobian_transposed.transpose()        return(jacobian)class Integration():    def __init__(self, expr, var):        self.expr = sym.sympify(expr)        self.var = sym.symbols(var)    def integrate(self, lower, upper, n=1, method='midpoint'):        user_input_fnc = sym.lambdify(self.var, self.expr)        if method.lower() == 'midpoint':            val =0            for i in range(n):                x_i = lower + (2 * i + 1) * (upper - lower) / (2 * n)                val += user_input_fnc(x_i)            return (upper - lower) / n * val        if method.lower() == 'trapezoid':            val = user_input_fnc(lower) + user_input_fnc(upper)            for i in range(1, n):                x_i = lower + i * (upper - lower) / n                val += 2 * user_input_fnc(x_i)            return 0.5 * (upper - lower) / n * val        if method.lower() == 'simpsons':            val = user_input_fnc(lower) + user_input_fnc(upper)            # add up all the odds            for i in range(1, 2 * n, 2):                x_i = lower + i * (upper - lower) / (2 * n)                val += 4 * user_input_fnc(x_i)            # add up all the evens            for i in range(2, 2 * n - 1, 2):                x_i = lower + i * (upper - lower) / (2 * n)                val += 2 * user_input_fnc(x_i)            return (upper - lower) / (6 * n) * valclass MLE():    '''    """Determininstic Dynamic Programming Steps"""    import numpy as np    # Step 1: Divi-up all possible pie sizes    N = 100    W_max, W_min = 1, 0.01    W = np.linspace(W_min, W_max, N)    # Step 2: Generate all possible states of the world (W-W')    W_possibles = np.tile(W.reshape((N, 1)), (1, N))    W_Wp = W_possibles - W_possibles.transpose()    # Step 3: Get rid of negatives    verysmallpos = 10 ** (-10)    nonpos = W_Wp <= 0    W_Wp[nonpos] = verysmallpos    # Step 4: Create initial value function (column of zeros)    vTp1 = np.array([0] * N)    # Step 5: Apply utility form to the possibility space    from numpy import log as ln    import math    from matplotlib import pyplot as plt    verylargeneg = (-1) * (10 ** 10)    def ln_w_np(xval):        try:            rtnval = ln(xval)        except ValueError:            rtnval = verylargeneg        return rtnval    def Ufn(mat):  # Utility function        return ln_w_np(mat)    # Step 6: Find value function and policy function at this iteration (figure 5.9).    def valpol(W_Wp, VW, beta=0.9):        ## evaluating utility function        U = Ufn(W_Wp)        ## repeating VW N times        VW_mat = np.tile(VW, (N, 1))        ## should not account for W-W' that is negative, so do this        ## putting in verylargeneg produced errors; try this instead        VW_mat[nonpos] = verylargeneg / 10000        ## contraction        contraction = U + (beta * VW_mat)        ## value function evaluation, one more iteration        ## note that W' was on the columns, so axis=1        VW_iter = contraction.max(axis=1)        ## policy function evaluation, one more iteration        polidx = np.argmax(contraction, axis=1)        pol_iter = W[polidx]        return VW_iter, pol_iter, polidx        # Might have to plot the policy function (value function) vs W today;    # Step 7: Define norm    def norm_val_fns(v1, v2):        err_msg = "v1 and v2 should be of the same length"        assert len(v1) == len(v2), err_msg        if type(v1) == list:            v1 = np.array(v1)        if type(v2) == list:            v2 = np.array(v2)        v1_2 = v1 - v2        mult = v1_2 * v1_2        return np.sum(mult)    # Step 8: Iterate it out    small = 1e-9    def dynprog(W_Wp, V, beta=0.9):        W_l = W_Wp.shape[0]        Wp_l = W_Wp.shape[1]        err_msg = "Dimensions should be the same"        assert W_l == Wp_l and Wp_l == len(V), err_msg        s = 1        inputV = V        d = small + 1        while d >= small:            opener = "Iteration " + str(s) + ":"            newV, newP, a = valpol(W_Wp, inputV, beta)            d = norm_val_fns(newV, inputV)            print(opener, "distance =", d)            inputV = newV            s += 1        print("-------------------")        print("Success at iteration " + str(s - 1) + ":", "distance =", d)        return newV, newP    val_cvg, pol_cvg = dynprog(W_Wp, vTp1)  # Example    # A note on log norms lognorm_dist = scipy.stats.lognorm(s=sigma, loc=0, scale=np.exp(mu))    """Stochastic Dynamic Programming Steps"""    # Step 1: Generate possible error space (Using the normal)    M = 7    sigma = 0.5    mu = sigma * 4    k = 3    from scipy.stats import norm as nr    import numpy as np    import math    def ncNormal(mu, sig, N, k):        """        Given mean and standard deviation of a normal distribution, along with        the number of nodes (to be equally spaced) and number of standard        deviations to be away from the mean, returns vector of weights and        vector of nodes for Newton-Cotes quadrature method.        input:        mu: mean of the normal distribution        sig: standard deviation of the normal distribution        N: number of nodes        k: number of standard deviations to be away from the mean        output:        tuple (double) containing        - vector of nodes        - vector of weights        """        # Lower and upper bounds (i.e. furthest nodes)        lb, ub = mu - sig * k, mu + sig * k        weights = []        nodes = np.linspace(lb, ub, N)        case_1 = nr.cdf((nodes[0] + nodes[1]) / 2, mu, sig)        case_N = 1 - nr.cdf((nodes[-1] + nodes[-2]) / 2, mu, sig)        for i in range(1, N + 1):            if i == 1:                weights.append(case_1)            elif i == N:                weights.append(case_N)            else:                now = nodes[i - 1]                fwd, bwd = nodes[i], nodes[i - 2]                z_min = (bwd + now) / 2                z_max = (fwd + now) / 2                weights.append(nr.cdf(z_max, mu, sig) - nr.cdf(z_min, mu, sig))        return np.array(nodes), np.array(weights)    e_vec, gamma_vec = ncNormal(mu, sigma, M, k)    # Step 2': Divi-up all possible pie sizes    N = 100    W_max, W_min = 1, 0.01    W = np.linspace(W_min, W_max, N)    # Step 3': Generate all possible states of the world (W-W')    W_possibles = np.tile(W.reshape((N, 1)), (1, N))    W_Wp = W_possibles - W_possibles.transpose()    # Step 4': Get rid of negatives    verysmallpos = 10 ** (-10)    nonpos = W_Wp <= 0    W_Wp[nonpos] = verysmallpos    # Step 5: generate terminal value function    VeTp1 = [0] * N    VeTp1 = [VeTp1] * M    VeTp1 = np.array(VeTp1)    # Step 6: calculate the value function and  **Note the ln_w_np() has been defined in part 1    def eU(W_Wp, evec):        eUmat = []        ## eUmat should result in dimensions x: N y: M z: N        for e in evec:            interm = e * ln_w_np(W_Wp)            eUmat.append(interm)        return np.array(eUmat)    def EV(V, probs, beta=0.9):        EVvec = []        ## each row is across different values of epsilon        ## for a certain value of W'        for i in range(N):            column = V[:, i]            Expec = np.sum(column * probs)            EVvec.append(Expec)        return np.array(EVvec)        ## Figure 5.10    def EV_to_cube(EV):        EV_cube = []        for i in range(M):            EV_slice = []            for j in range(N):                EV_slice.append(EV)            EV_slice = np.array(EV_slice)            EV_slice[nonpos] = verylargeneg / 10000            EV_cube.append(EV_slice)        return np.array(EV_cube)    def eU_bEV(eU_cube, EV_cube, beta=0.9):        entire = eU_cube + beta * EV_cube        Vprimes = []        Wprimes = []        for i in range(M):            contraction_e = entire[i]            val_iter = contraction_e.max(axis=1)            polidx = np.argmax(contraction_e, axis=1)            pol_iter = W[polidx]            Vprimes.append(val_iter)            Wprimes.append(pol_iter)        return np.array(Vprimes), np.array(Wprimes)        ## Million dollar    # Step 7: Find value function and policy function    def vp_stoch(W_Wp, VWe, evec, probs, beta=0.9):        eUcube = eU(W_Wp, evec)        EVvec = EV(VWe, probs)        EVcube = EV_to_cube(EVvec)        return eU_bEV(eUcube, EVcube)    VeT, PeT = vp_stoch(W_Wp, VeTp1, e_vec, gamma_vec)    # Step 8:  Vectorize and find norm    def vectorize(mat):        mat = mat.transpose()        vector = []        for row in mat:            vector += list(row)        return np.array(vector)    def norm_val_stoch(v_T, v_Tp1):        err_msg = "The two inputs must have the same dimensions"        assert v_T.shape == v_Tp1.shape, err_msg        if type(v_T) == list:            v_T = np.array(v_T)        if type(v_Tp1) == list:            v_Tp1 = np.array(v_Tp1)        vecT = vectorize(v_T)        vecTp1 = vectorize(v_Tp1)        vec_diff = vecT - vecTp1        mult_vec = vec_diff * vec_diff        return np.sum(mult_vec)    d_T = norm_val_stoch(VeT, VeTp1)    # Step 9: Iterate    small = 1e-9    def dynprog_stoch(W_Wp, VWe, evec, probs, beta=0.9):        W_l = W_Wp.shape[0]        Wp_l = W_Wp.shape[1]        V_l = VWe.shape[1]        err_msg = "Dimensions should be the same"        assert W_l == Wp_l and Wp_l == V_l, err_msg        s = 1        inputVWe = VWe        d = small + 1        while d >= small:            opener = "Iteration " + str(s) + ":"            newVWe, newPWe = vp_stoch(W_Wp, inputVWe, evec, probs)            d = norm_val_stoch(newVWe, inputVWe)            print(opener, "distance =", d)            inputVWe = newVWe            s += 1        print("-------------------")        print("Success at iteration " + str(s - 1) + ":", "distance =", d)        return newVWe, newPWe    val_cvg, pol_cvg = dynprog_stoch(W_Wp, VeTp1, e_vec, gamma_vec)    # Step 10: Graph!    import matplotlib.pyplot as plt    from mpl_toolkits.mplot3d import Axes3D    X = W    Y = e_vec    X, Y = np.meshgrid(X, Y)    Z = pol_cvg    fig = plt.figure()    ax = fig.add_subplot(111, projection='3d')    ax.plot_surface(Y, X, Z)    ax.set_title("Converged policy function")    ax.set_zlabel("Cake tomorrow")    ax.set_ylabel("Cake today")    ax.set_xlabel("Shock on utility")    plt.show()    '''    passif __name__ == '__main__':    part1 = Differentiation('(sin(x) + 1)**(sin((cos(x))))', 'x')    part2 = Integration('0.1*x**4 - 1.5*x**3 + 0.53*x**2 + 2*x + 1', 'x')